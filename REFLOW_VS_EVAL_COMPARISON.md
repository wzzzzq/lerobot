# Reflow Training vs RobotWin Eval æ•°æ®å¤„ç†å¯¹æ¯”åˆ†æ

## é—®é¢˜æè¿°
Velocityè´¨é‡æµ‹è¯•æ˜¾ç¤ºstudentæ¨¡å‹é¢„æµ‹è¯¯å·®åªæœ‰4.5%ï¼ˆè‰¯å¥½ï¼‰ï¼Œä½†åœ¨robotwin evalæ—¶æœºæ¢°è‡‚å®Œå…¨ä¹±åŠ¨ã€‚è¿™è¡¨æ˜è®­ç»ƒå’Œinferenceåœ¨æ•°æ®å¤„ç†ä¸Šå­˜åœ¨ä¸ä¸€è‡´ã€‚

## è¯¦ç»†å¯¹æ¯”

### 1. å›¾ç‰‡å¤„ç†æµç¨‹

#### **Evalè„šæœ¬** (`examples/robotwin_eval/eval_policy_smolvla.py`)
```python
# Line 63-67: prepare_imgå‡½æ•°
def prepare_img(img):
    # Convert HWC to CHW, normalize to [0, 1]
    img = np.transpose(img, (2, 0, 1))  # HWC â†’ CHW
    img = img.astype(np.float32) / 255.0  # [0, 255] â†’ [0, 1]
    return torch.from_numpy(img)

# Line 88: ä¼ ç»™preprocessor
self.observation_window = self.preprocessor(observation)
```

#### **Policyçš„prepare_images** (`src/lerobot/policies/smolvla/modeling_smolvla.py:344-384`)
```python
def prepare_images(self, batch):
    for key in present_img_keys:
        img = batch[key][:, -1, :, :, :] if batch[key].ndim == 5 else batch[key]

        # Resize with padding
        if self.config.resize_imgs_with_padding is not None:
            img = resize_with_pad(img, *self.config.resize_imgs_with_padding, pad_value=0)

        # âš ï¸ å…³é”®: ä»[0,1]å½’ä¸€åŒ–åˆ°[-1,1] (SigLIPè¦æ±‚)
        img = img * 2.0 - 1.0  # [0, 1] â†’ [-1, 1]
```

**æµç¨‹**:
- Eval: ç¯å¢ƒRGB [0,255] â†’ prepare_img â†’ [0,1] CHW â†’ preprocessor â†’ prepare_images â†’ **[-1,1]**
- Training: Datasetå·²ç»æ˜¯[0,1] CHW â†’ prepare_images â†’ **[-1,1]**

**âœ“ è¿™ä¸ªåº”è¯¥æ˜¯ä¸€è‡´çš„**

---

### 2. ç›¸æœºé¡ºåº

#### **Evalè„šæœ¬**
```python
# Line 78-84: Camera names - MUST match training data order
camera_names = ["head_camera", "left_camera", "right_camera"]
for i, camera_name in enumerate(camera_names):
    if i < len(img_arr):
        key = f"observation.images.{camera_name}"
        observation[key] = prepare_img(img_arr[i])

# Line 182-186: encode_obsæå–ç›¸æœº
input_rgb_arr = [
    observation["observation"]["head_camera"]["rgb"],  # 0
    observation["observation"]["left_camera"]["rgb"],   # 1
    observation["observation"]["right_camera"]["rgb"],  # 2
]
```

#### **Training Dataset**
éœ€è¦æ£€æŸ¥æ•°æ®é›†ä¸­ç›¸æœºçš„é¡ºåºï¼

**â“ éœ€è¦éªŒè¯**: Datasetä¸­çš„ç›¸æœºé¡ºåºæ˜¯å¦å’Œevalå®Œå…¨ä¸€è‡´ï¼Ÿ
- Dataset keys: `observation.images.head_camera`, `observation.images.left_camera`, `observation.images.right_camera`
- Policy configä¸­çš„ `image_features` é¡ºåºæ˜¯ä»€ä¹ˆï¼Ÿ

---

### 3. Stateå¤„ç†

#### **Evalè„šæœ¬**
```python
# Line 69: ç›´æ¥è½¬float32
state_tensor = torch.from_numpy(np.array(state, dtype=np.float32))

# Line 73: æ·»åŠ åˆ°observation
observation = {
    "observation.state": state_tensor,
    ...
}
```

#### **Policyçš„prepare_state** (`modeling_smolvla.py:413-417`)
```python
def prepare_state(self, batch):
    """Pad state"""
    state = batch[OBS_STATE][:, -1, :] if batch[OBS_STATE].ndim > 2 else batch[OBS_STATE]
    state = pad_vector(state, self.config.max_state_dim)  # Padåˆ°max_state_dim
    return state
```

**æµç¨‹**:
- Eval: state (14-dim) â†’ preprocessor (å¯èƒ½normalize) â†’ prepare_state â†’ **padded state**
- Training: batch["observation.state"] â†’ prepare_state â†’ **padded state**

**â“ éœ€è¦éªŒè¯**:
1. Evalä¼ å…¥çš„stateç»´åº¦æ˜¯å¦æ­£ç¡®ï¼ˆ14-dimï¼‰ï¼Ÿ
2. Stateæ˜¯å¦éœ€è¦å½’ä¸€åŒ–ï¼ŸPreprocessorçš„NormalizerProcessorStepä¼šå¤„ç†stateå—ï¼Ÿ

---

### 4. Languageå¤„ç†

#### **Evalè„šæœ¬**
```python
# Line 74: ä½¿ç”¨instructionå­—ç¬¦ä¸²
observation = {
    "task": self.instruction if isinstance(self.instruction, str) else self.instruction[0],
}

# Line 88: Preprocessorä¼štokenize
self.observation_window = self.preprocessor(observation)
```

#### **Training**
```python
# prepare_reflow_batch (lerobot_train_reflow.py:207-208)
lang_tokens = batch[f"{OBS_LANGUAGE_TOKENS}"]  # å·²ç»tokenized
lang_masks = batch[f"{OBS_LANGUAGE_ATTENTION_MASK}"]
```

#### **Preprocessor** (`processor_smolvla.py:73-78`)
```python
SmolVLANewLineProcessor(),  # æ·»åŠ newline
TokenizerProcessorStep(
    tokenizer_name=config.vlm_model_name,
    padding=config.pad_language_to,
    padding_side="right",
    max_length=config.tokenizer_max_length,
),
```

**âœ“ è¿™ä¸ªåº”è¯¥æ˜¯ä¸€è‡´çš„**: evalä½¿ç”¨task string â†’ preprocessor tokenizeï¼Œtrainingä½¿ç”¨datasetçš„task â†’ preprocessor tokenize

---

### 5. Inferenceæ–¹æ³•å¯¹æ¯”

#### **Evalè„šæœ¬** (`eval_policy_smolvla.py:101`)
```python
# select_action
action_tensor = self.policy.select_action(self.observation_window)
```

#### **select_actionæµç¨‹** (`modeling_smolvla.py:291-311`)
```python
def select_action(self, batch: dict[str, Tensor], noise: Tensor | None = None) -> Tensor:
    self.eval()
    batch = self._prepare_batch(batch)  # adapt_to_pi_aloha decode

    if len(self._queues[ACTION]) == 0:
        actions = self._get_action_chunk(batch, noise)
        # (batch_size, n_action_steps, action_dim)
        self._queues[ACTION].extend(actions.transpose(0, 1)[: self.config.n_action_steps])

    return self._queues[ACTION].popleft()
```

#### **_get_action_chunk** (`modeling_smolvla.py:248-272`)
```python
def _get_action_chunk(self, batch: dict[str, Tensor], noise: Tensor | None = None) -> Tensor:
    images, img_masks = self.prepare_images(batch)
    state = self.prepare_state(batch)
    lang_tokens = batch[f"{OBS_LANGUAGE_TOKENS}"]
    lang_masks = batch[f"{OBS_LANGUAGE_ATTENTION_MASK}"]

    # âš ï¸ ä½¿ç”¨sample_actions (ODE integration)
    actions = self.model.sample_actions(images, img_masks, lang_tokens, lang_masks, state, noise=noise)

    # âš ï¸âš ï¸âš ï¸ å…³é”®: Unpad actionsåˆ°original_action_dim (14)
    original_action_dim = self.config.action_feature.shape[0]
    actions = actions[:, :, :original_action_dim]  # 32 â†’ 14

    if self.config.adapt_to_pi_aloha:
        actions = self._pi_aloha_encode_actions(actions)

    return actions
```

#### **Reflow Training** (`lerobot_train_reflow.py:223-229`)
```python
# Teacherç”ŸæˆX_0
X_0 = teacher.model.sample_actions(
    images, img_masks, lang_tokens, lang_masks, state, noise=X_1_padded
)
# CRITICAL FIX: Do NOT unpad X_0!
# ä¿æŒX_0ä¸º32-dim
```

**âœ“ è¿™é‡Œåº”è¯¥æ²¡é—®é¢˜**:
- Training: teacherç”Ÿæˆ32-dim X_0 â†’ studentå­¦ä¹ velocity â†’ è¾“å‡º32-dim
- Inference: studentç”Ÿæˆ32-dim â†’ unpadåˆ°14-dim â†’ è¿”å›ç»™ç¯å¢ƒ

Unpadåªæ˜¯å»æ‰paddingç»´åº¦ï¼ˆåº”è¯¥æ¥è¿‘0ï¼‰ï¼Œä¸å½±å“å‰14ç»´çš„å€¼ã€‚

---

### 6. Preprocessor/Postprocessorä½¿ç”¨

#### **Evalè„šæœ¬** (`eval_policy_smolvla.py:165-169`)
```python
preprocessor, postprocessor = make_pre_post_processors(
    policy_cfg=policy.config,
    pretrained_path=policy_path,  # ä»checkpointåŠ è½½
)
```

#### **Preprocessor Pipeline** (`processor_smolvla.py:69-85`)
```python
input_steps = [
    RenameObservationsProcessorStep(rename_map={}),
    AddBatchDimensionProcessorStep(),  # æ·»åŠ batchç»´åº¦
    SmolVLANewLineProcessor(),         # taskæ·»åŠ \n
    TokenizerProcessorStep(...),       # tokenize task
    DeviceProcessorStep(device=config.device),  # ç§»åˆ°GPU
    NormalizerProcessorStep(          # âš ï¸ å½’ä¸€åŒ–stateå’Œaction
        features={**config.input_features, **config.output_features},
        norm_map=config.normalization_mapping,
        stats=dataset_stats,
    ),
]
```

**â“â“â“ å…³é”®é—®é¢˜**:
1. **NormalizerProcessorStepä¼šå½’ä¸€åŒ–stateå—ï¼Ÿ**
   - å¦‚æœä¼šï¼Œevalçš„stateéœ€è¦å…ˆå½’ä¸€åŒ–å†ä¼ å…¥
   - Trainingçš„stateä»datasetæ¥ï¼Œå¯èƒ½å·²ç»å½’ä¸€åŒ–äº†å—ï¼Ÿ

2. **Datasetè¿”å›çš„stateæ˜¯å¦å·²ç»å½’ä¸€åŒ–ï¼Ÿ**
   - éœ€è¦æ£€æŸ¥datasetåŠ è½½æ—¶æ˜¯å¦åº”ç”¨äº†normalization

---

### 7. adapt_to_pi_alohaå¤„ç†

#### **Inference** (`modeling_smolvla.py:274-277, 269-270`)
```python
def _prepare_batch(self, batch: dict[str, Tensor]) -> dict[str, Tensor]:
    if self.config.adapt_to_pi_aloha:
        batch[OBS_STATE] = self._pi_aloha_decode_state(batch[OBS_STATE])
    return batch

# In _get_action_chunk:
if self.config.adapt_to_pi_aloha:
    actions = self._pi_aloha_encode_actions(actions)
```

#### **Training** (`lerobot_train_reflow.py:200-202`)
```python
if student.config.adapt_to_pi_aloha:
    batch[OBS_STATE] = student._pi_aloha_decode_state(batch[OBS_STATE])
    batch[ACTION] = student._pi_aloha_encode_actions_inv(batch[ACTION])
```

**âœ“ è¿™ä¸ªåº”è¯¥æ˜¯ä¸€è‡´çš„**

---

## ğŸš¨ å…³é”®å¯ç–‘ç‚¹æ€»ç»“

### 1. **Stateå½’ä¸€åŒ–** (æœ€å¯ç–‘!)
- **é—®é¢˜**: NormalizerProcessorStepå¯èƒ½ä¼šå½’ä¸€åŒ–state
- **å½±å“**: å¦‚æœtrainingæ—¶stateå·²å½’ä¸€åŒ–ï¼Œä½†evalæ—¶stateæ˜¯åŸå§‹å€¼ï¼Œä¼šå¯¼è‡´å®Œå…¨é”™è¯¯çš„é¢„æµ‹
- **éªŒè¯æ–¹æ³•**:
  1. æ£€æŸ¥policy configä¸­çš„`normalization_mapping`å’Œ`input_features`
  2. æ‰“å°evalæ—¶preprocessorå‰åçš„stateå€¼èŒƒå›´
  3. å¯¹æ¯”training dataloaderè¿”å›çš„stateå€¼èŒƒå›´

### 2. **ç›¸æœºé¡ºåº** (éœ€è¦éªŒè¯)
- **é—®é¢˜**: Policy configä¸­çš„`image_features`é¡ºåºå¯èƒ½å’Œevalä¸ä¸€è‡´
- **å½±å“**: ç›¸æœºfeedé”™ä½ä¼šå¯¼è‡´æ¨¡å‹çœ‹åˆ°é”™è¯¯çš„è§†è§’
- **éªŒè¯æ–¹æ³•**:
  1. æ£€æŸ¥policy configçš„`image_features`åˆ—è¡¨é¡ºåº
  2. æ£€æŸ¥datasetä¸­ç›¸æœºçš„é¡ºåº
  3. å¯¹æ¯”evalä¸­camera_namesåˆ—è¡¨

### 3. **Batch dimension** (preprocessorå¤„ç†)
- **é—®é¢˜**: AddBatchDimensionProcessorStepä¼šæ·»åŠ batchç»´åº¦
- **å½±å“**: å¦‚æœstateæ˜¯(14,)ï¼Œä¼šå˜æˆ(1, 14)ï¼›ä½†prepare_stateæœŸæœ›(batch, 14)
- **éªŒè¯æ–¹æ³•**: æ‰“å°preprocessorè¾“å‡ºçš„observation windowçš„shapes

---

## å»ºè®®çš„Debugæ­¥éª¤

### Step 1: åˆ›å»ºå¯¹æ¯”è„šæœ¬
åˆ›å»ºä¸€ä¸ªè„šæœ¬åŒæ—¶è¿è¡Œtrainingçš„prepare_reflow_batchå’Œevalçš„observationå¤„ç†ï¼Œå¯¹æ¯”ï¼š
- Images shapeå’Œvalue range
- State shapeå’Œvalue range
- Language tokens
- æœ€ç»ˆä¼ ç»™model.sample_actionsçš„æ‰€æœ‰inputs

### Step 2: æ£€æŸ¥Config
æ‰“å°policy.configä¸­çš„ï¼š
- `normalization_mapping`
- `input_features`
- `image_features`çš„é¡ºåº
- `adapt_to_pi_aloha`
- `resize_imgs_with_padding`

### Step 3: æ·»åŠ Debugæ—¥å¿—åˆ°Eval
åœ¨evalè„šæœ¬ä¸­æ·»åŠ æ—¥å¿—ï¼š
```python
# After preprocessor
print("Preprocessed observation:")
for k, v in self.observation_window.items():
    if isinstance(v, torch.Tensor):
        print(f"  {k}: shape={v.shape}, min={v.min():.3f}, max={v.max():.3f}, mean={v.mean():.3f}")

# Before sample_actions in _get_action_chunk
print("Inputs to sample_actions:")
print(f"  images: {len(images)} cameras, shape={images[0].shape}, range=[{images[0].min():.3f}, {images[0].max():.3f}]")
print(f"  state: shape={state.shape}, range=[{state.min():.3f}, {state.max():.3f}]")
print(f"  lang_tokens: shape={lang_tokens.shape}")
```

### Step 4: å¯¹æ¯”Dataset vs Eval
ä»training datasetå–ä¸€ä¸ªbatchï¼Œæ‰“å°ï¼š
- batch["observation.state"]çš„shapeå’Œrange
- batch["observation.images.xxx"]çš„shapeå’Œrange
- å’Œevalä¸­å¯¹åº”å€¼å¯¹æ¯”

---

## æœ€å¯èƒ½çš„æ ¹æœ¬åŸå› 

åŸºäºä»£ç åˆ†æï¼Œ**æœ€å¯èƒ½çš„é—®é¢˜æ˜¯Stateå½’ä¸€åŒ–ä¸ä¸€è‡´**ï¼š

1. Training datasetå¯èƒ½è¿”å›å½’ä¸€åŒ–åçš„state (æ¯”å¦‚[-1, 1]æˆ–[0, 1])
2. Evalä¼ å…¥çš„æ˜¯åŸå§‹joint state (æ¯”å¦‚[-3.14, 3.14]ç­‰ç‰©ç†å•ä½)
3. å³ä½¿preprocessoræœ‰NormalizerProcessorStepï¼Œå®ƒå¯èƒ½éœ€è¦æ­£ç¡®çš„statsæ‰èƒ½å½’ä¸€åŒ–

**éªŒè¯**: å¯¹æ¯”trainingæ—¶batch["observation.state"]å’Œevalæ—¶observation["observation.state"]çš„æ•°å€¼èŒƒå›´ã€‚
