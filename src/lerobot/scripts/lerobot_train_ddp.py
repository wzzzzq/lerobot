#!/usr/bin/env python

# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Multi-GPU training script using PyTorch native DDP (DistributedDataParallel).

Usage:
    # Single GPU
    python src/lerobot/scripts/lerobot_train_ddp.py [args]

    # Multi-GPU (e.g., 4 GPUs)
    torchrun --nproc_per_node=4 src/lerobot/scripts/lerobot_train_ddp.py [args]

    # Or using torch.distributed.launch (deprecated but still works)
    python -m torch.distributed.launch --nproc_per_node=4 src/lerobot/scripts/lerobot_train_ddp.py [args]
"""

import logging
import os
import time
from contextlib import nullcontext
from pprint import pformat
from typing import Any

import torch
import torch.distributed as dist
from termcolor import colored
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import Optimizer
from torch.utils.data.distributed import DistributedSampler

from lerobot.configs import parser
from lerobot.configs.train import TrainPipelineConfig
from lerobot.datasets.factory import make_dataset
from lerobot.datasets.sampler import EpisodeAwareSampler
from lerobot.datasets.utils import cycle
from lerobot.envs.factory import make_env
from lerobot.envs.utils import close_envs
from lerobot.optim.factory import make_optimizer_and_scheduler
from lerobot.policies.factory import make_policy, make_pre_post_processors
from lerobot.policies.pretrained import PreTrainedPolicy
from lerobot.rl.wandb_utils import WandBLogger
from lerobot.scripts.lerobot_eval import eval_policy_all
from lerobot.utils.logging_utils import AverageMeter, MetricsTracker
from lerobot.utils.random_utils import set_seed
from lerobot.utils.train_utils import (
    get_step_checkpoint_dir,
    get_step_identifier,
    load_training_state,
    save_checkpoint,
    update_last_checkpoint,
)
from lerobot.utils.utils import (
    format_big_number,
    has_method,
    init_logging,
)


def setup_ddp():
    """Initialize the distributed process group."""
    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
    else:
        # Single GPU mode
        rank = 0
        world_size = 1
        local_rank = 0

    if world_size > 1:
        dist.init_process_group(backend="nccl")
        torch.cuda.set_device(local_rank)

    return rank, world_size, local_rank


def cleanup_ddp():
    """Clean up the distributed process group."""
    if dist.is_initialized():
        dist.destroy_process_group()


def reduce_tensor(tensor: torch.Tensor, world_size: int) -> torch.Tensor:
    """Reduce tensor across all processes."""
    if world_size == 1:
        return tensor
    rt = tensor.clone()
    dist.all_reduce(rt, op=dist.ReduceOp.SUM)
    rt /= world_size
    return rt


def update_policy(
    train_metrics: MetricsTracker,
    policy: PreTrainedPolicy,
    batch: Any,
    optimizer: Optimizer,
    grad_clip_norm: float,
    rank: int,
    world_size: int,
    lr_scheduler=None,
    use_amp: bool = False,
    scaler=None,
) -> tuple[MetricsTracker, dict]:
    """
    Performs a single training step to update the policy's weights.

    Args:
        train_metrics: A MetricsTracker instance to record training statistics.
        policy: The policy model to be trained (should be wrapped with DDP for multi-GPU).
        batch: A batch of training data.
        optimizer: The optimizer used to update the policy's parameters.
        grad_clip_norm: The maximum norm for gradient clipping.
        rank: Process rank (0 for main process).
        world_size: Total number of processes.
        lr_scheduler: An optional learning rate scheduler.
        use_amp: Whether to use automatic mixed precision.
        scaler: GradScaler for mixed precision training.

    Returns:
        A tuple containing:
        - The updated MetricsTracker with new statistics for this step.
        - A dictionary of outputs from the policy's forward pass, for logging purposes.
    """
    start_time = time.perf_counter()
    policy.train()

    optimizer.zero_grad()

    # Forward pass with optional mixed precision
    if use_amp and scaler is not None:
        with torch.cuda.amp.autocast():
            loss, output_dict = policy.forward(batch)
        scaler.scale(loss).backward()
    else:
        loss, output_dict = policy.forward(batch)
        loss.backward()

    # Clip gradients
    if use_amp and scaler is not None:
        scaler.unscale_(optimizer)

    if grad_clip_norm > 0:
        # Get the underlying model (unwrap DDP if needed)
        model_to_clip = policy.module if isinstance(policy, DDP) else policy
        grad_norm = torch.nn.utils.clip_grad_norm_(model_to_clip.parameters(), grad_clip_norm)
    else:
        model_to_clip = policy.module if isinstance(policy, DDP) else policy
        grad_norm = torch.nn.utils.clip_grad_norm_(
            model_to_clip.parameters(), float("inf"), error_if_nonfinite=False
        )

    # Optimizer step
    if use_amp and scaler is not None:
        scaler.step(optimizer)
        scaler.update()
    else:
        optimizer.step()

    # Step learning rate scheduler
    if lr_scheduler is not None:
        lr_scheduler.step()

    # Update internal buffers if policy has update method
    model_to_update = policy.module if isinstance(policy, DDP) else policy
    if has_method(model_to_update, "update"):
        model_to_update.update()

    # Reduce metrics across all processes
    if world_size > 1:
        loss_tensor = reduce_tensor(loss.detach(), world_size)
        grad_norm_tensor = reduce_tensor(torch.tensor(grad_norm).to(loss.device), world_size)
        train_metrics.loss = loss_tensor.item()
        train_metrics.grad_norm = grad_norm_tensor.item()
    else:
        train_metrics.loss = loss.item()
        train_metrics.grad_norm = grad_norm.item()

    train_metrics.lr = optimizer.param_groups[0]["lr"]
    train_metrics.update_s = time.perf_counter() - start_time

    return train_metrics, output_dict


@parser.wrap()
def train(cfg: TrainPipelineConfig):
    """
    Main function to train a policy with multi-GPU support using PyTorch DDP.

    This function orchestrates the entire training pipeline with native PyTorch DDP:
    - Setting up distributed training environment
    - Creating dataset, policy, and optimizer
    - Running the main training loop with gradient synchronization
    - Logging metrics and saving checkpoints (only on main process)

    Args:
        cfg: A `TrainPipelineConfig` object containing all training configurations.
    """
    cfg.validate()

    # Setup distributed training
    rank, world_size, local_rank = setup_ddp()
    is_main_process = rank == 0

    # Set device
    if world_size > 1:
        device = torch.device(f"cuda:{local_rank}")
    else:
        device = torch.device(cfg.policy.device if hasattr(cfg.policy, 'device') else "cuda:0")

    # Initialize logging only on main process
    if is_main_process:
        init_logging()
        logging.info(f"Running on {world_size} GPU(s)")
        logging.info(pformat(cfg.to_dict()))

    # Initialize wandb only on main process
    wandb_logger = None
    if cfg.wandb.enable and cfg.wandb.project and is_main_process:
        wandb_logger = WandBLogger(cfg)
    elif is_main_process:
        logging.info(colored("Logs will be saved locally.", "yellow", attrs=["bold"]))

    # Set random seed
    if cfg.seed is not None:
        # Add rank to seed to ensure different data shuffling per process
        seed = cfg.seed + rank
        set_seed(seed)

    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True

    # Dataset loading synchronization
    if is_main_process:
        logging.info("Creating dataset")
        dataset = make_dataset(cfg)

    if world_size > 1:
        dist.barrier()

    if not is_main_process:
        dataset = make_dataset(cfg)

    # Create evaluation environment (only on main process for eval)
    eval_env = None
    if cfg.eval_freq > 0 and cfg.env is not None and is_main_process:
        logging.info("Creating env")
        eval_env = make_env(cfg.env, n_envs=cfg.eval.batch_size, use_async_envs=cfg.eval.use_async_envs)

    # Create policy
    if is_main_process:
        logging.info("Creating policy")

    policy = make_policy(
        cfg=cfg.policy,
        ds_meta=dataset.meta,
        rename_map=cfg.rename_map,
    )
    policy = policy.to(device)

    # Wrap policy with DDP for multi-GPU training
    if world_size > 1:
        if is_main_process:
            logging.info("Wrapping policy with DistributedDataParallel")
        policy = DDP(
            policy,
            device_ids=[local_rank],
            output_device=local_rank,
            find_unused_parameters=True,  # For models with conditional computation
        )

    if world_size > 1:
        dist.barrier()

    # Create processors
    processor_kwargs = {}
    postprocessor_kwargs = {}
    if (cfg.policy.pretrained_path and not cfg.resume) or not cfg.policy.pretrained_path:
        processor_kwargs["dataset_stats"] = dataset.meta.stats

    if cfg.policy.pretrained_path is not None:
        # Get config from the underlying model if wrapped with DDP
        policy_config = policy.module.config if isinstance(policy, DDP) else policy.config

        processor_kwargs["preprocessor_overrides"] = {
            "device_processor": {"device": device.type},
            "normalizer_processor": {
                "stats": dataset.meta.stats,
                "features": {**policy_config.input_features, **policy_config.output_features},
                "norm_map": policy_config.normalization_mapping,
            },
        }
        processor_kwargs["preprocessor_overrides"]["rename_observations_processor"] = {
            "rename_map": cfg.rename_map
        }
        postprocessor_kwargs["postprocessor_overrides"] = {
            "unnormalizer_processor": {
                "stats": dataset.meta.stats,
                "features": policy_config.output_features,
                "norm_map": policy_config.normalization_mapping,
            },
        }

    preprocessor, postprocessor = make_pre_post_processors(
        policy_cfg=cfg.policy,
        pretrained_path=cfg.policy.pretrained_path,
        **processor_kwargs,
        **postprocessor_kwargs,
    )

    # Create optimizer and scheduler
    if is_main_process:
        logging.info("Creating optimizer and scheduler")

    # Pass the underlying model to optimizer (unwrap DDP)
    model_for_optim = policy.module if isinstance(policy, DDP) else policy
    optimizer, lr_scheduler = make_optimizer_and_scheduler(cfg, model_for_optim)

    step = 0
    if cfg.resume:
        step, optimizer, lr_scheduler = load_training_state(cfg.checkpoint_path, optimizer, lr_scheduler)

    # Log training info
    num_learnable_params = sum(p.numel() for p in model_for_optim.parameters() if p.requires_grad)
    num_total_params = sum(p.numel() for p in model_for_optim.parameters())

    if is_main_process:
        logging.info(colored("Output dir:", "yellow", attrs=["bold"]) + f" {cfg.output_dir}")
        if cfg.env is not None:
            logging.info(f"{cfg.env.task=}")
        logging.info(f"{cfg.steps=} ({format_big_number(cfg.steps)})")
        logging.info(f"{dataset.num_frames=} ({format_big_number(dataset.num_frames)})")
        logging.info(f"{dataset.num_episodes=}")
        effective_bs = cfg.batch_size * world_size
        logging.info(f"Effective batch size: {cfg.batch_size} x {world_size} = {effective_bs}")
        logging.info(f"{num_learnable_params=} ({format_big_number(num_learnable_params)})")
        logging.info(f"{num_total_params=} ({format_big_number(num_total_params)})")

    # Create dataloader with DistributedSampler for multi-GPU
    if hasattr(cfg.policy, "drop_n_last_frames"):
        # Note: EpisodeAwareSampler doesn't support distributed training out of the box
        # You may need to modify it or use a different approach
        if world_size > 1:
            logging.warning(
                "EpisodeAwareSampler with drop_n_last_frames is not fully compatible with DDP. "
                "Falling back to DistributedSampler."
            )
        sampler = DistributedSampler(
            dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True,
            seed=cfg.seed if cfg.seed is not None else 0,
        ) if world_size > 1 else None
        shuffle = sampler is None
    else:
        sampler = DistributedSampler(
            dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True,
            seed=cfg.seed if cfg.seed is not None else 0,
        ) if world_size > 1 else None
        shuffle = sampler is None and not cfg.dataset.streaming

    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=cfg.num_workers,
        batch_size=cfg.batch_size,
        shuffle=shuffle,
        sampler=sampler,
        pin_memory=device.type == "cuda",
        drop_last=False,
        prefetch_factor=2 if cfg.num_workers > 0 else None,
    )

    dl_iter = cycle(dataloader)
    policy.train()

    # Setup automatic mixed precision
    use_amp = cfg.policy.device == "cuda" and hasattr(cfg, "use_amp") and cfg.use_amp
    scaler = torch.cuda.amp.GradScaler() if use_amp else None

    # Training metrics
    train_metrics = {
        "loss": AverageMeter("loss", ":.3f"),
        "grad_norm": AverageMeter("grdn", ":.3f"),
        "lr": AverageMeter("lr", ":0.1e"),
        "update_s": AverageMeter("updt_s", ":.3f"),
        "dataloading_s": AverageMeter("data_s", ":.3f"),
    }

    effective_batch_size = cfg.batch_size * world_size
    train_tracker = MetricsTracker(
        effective_batch_size,
        dataset.num_frames,
        dataset.num_episodes,
        train_metrics,
        initial_step=step,
    )

    if is_main_process:
        logging.info("Start offline training on a fixed dataset")

    # Main training loop
    for _ in range(step, cfg.steps):
        start_time = time.perf_counter()
        batch = next(dl_iter)
        batch = preprocessor(batch)
        train_tracker.dataloading_s = time.perf_counter() - start_time

        train_tracker, output_dict = update_policy(
            train_tracker,
            policy,
            batch,
            optimizer,
            cfg.optimizer.grad_clip_norm,
            rank=rank,
            world_size=world_size,
            lr_scheduler=lr_scheduler,
            use_amp=use_amp,
            scaler=scaler,
        )

        step += 1
        train_tracker.step()

        is_log_step = cfg.log_freq > 0 and step % cfg.log_freq == 0 and is_main_process
        is_saving_step = step % cfg.save_freq == 0 or step == cfg.steps
        is_eval_step = cfg.eval_freq > 0 and step % cfg.eval_freq == 0

        if is_log_step:
            logging.info(train_tracker)
            if wandb_logger:
                wandb_log_dict = train_tracker.to_dict()
                if output_dict:
                    wandb_log_dict.update(output_dict)
                wandb_logger.log_dict(wandb_log_dict, step)
            train_tracker.reset_averages()

        if cfg.save_checkpoint and is_saving_step:
            if is_main_process:
                logging.info(f"Checkpoint policy after step {step}")
                checkpoint_dir = get_step_checkpoint_dir(cfg.output_dir, cfg.steps, step)

                # Unwrap DDP before saving
                policy_to_save = policy.module if isinstance(policy, DDP) else policy

                save_checkpoint(
                    checkpoint_dir=checkpoint_dir,
                    step=step,
                    cfg=cfg,
                    policy=policy_to_save,
                    optimizer=optimizer,
                    scheduler=lr_scheduler,
                    preprocessor=preprocessor,
                    postprocessor=postprocessor,
                )
                update_last_checkpoint(checkpoint_dir)
                if wandb_logger:
                    wandb_logger.log_policy(checkpoint_dir)

            if world_size > 1:
                dist.barrier()

        if cfg.env and is_eval_step and is_main_process:
            step_id = get_step_identifier(step, cfg.steps)
            logging.info(f"Eval policy at step {step}")

            # Unwrap DDP for evaluation
            policy_to_eval = policy.module if isinstance(policy, DDP) else policy

            with torch.no_grad():
                eval_info = eval_policy_all(
                    envs=eval_env,
                    policy=policy_to_eval,
                    preprocessor=preprocessor,
                    postprocessor=postprocessor,
                    n_episodes=cfg.eval.n_episodes,
                    videos_dir=cfg.output_dir / "eval" / f"videos_step_{step_id}",
                    max_episodes_rendered=4,
                    start_seed=cfg.seed,
                    max_parallel_tasks=cfg.env.max_parallel_tasks,
                )

            aggregated = eval_info["overall"]

            for suite, suite_info in eval_info.items():
                logging.info("Suite %s aggregated: %s", suite, suite_info)

            eval_metrics = {
                "avg_sum_reward": AverageMeter("âˆ‘rwrd", ":.3f"),
                "pc_success": AverageMeter("success", ":.1f"),
                "eval_s": AverageMeter("eval_s", ":.3f"),
            }
            eval_tracker = MetricsTracker(
                cfg.batch_size,
                dataset.num_frames,
                dataset.num_episodes,
                eval_metrics,
                initial_step=step,
            )
            eval_tracker.eval_s = aggregated.pop("eval_s")
            eval_tracker.avg_sum_reward = aggregated.pop("avg_sum_reward")
            eval_tracker.pc_success = aggregated.pop("pc_success")

            if wandb_logger:
                wandb_log_dict = {**eval_tracker.to_dict(), **eval_info}
                wandb_logger.log_dict(wandb_log_dict, step, mode="eval")
                wandb_logger.log_video(eval_info["overall"]["video_paths"][0], step, mode="eval")

        if world_size > 1:
            dist.barrier()

    # Cleanup
    if eval_env:
        close_envs(eval_env)

    if is_main_process:
        logging.info("End of training")

        if cfg.policy.push_to_hub:
            policy_to_push = policy.module if isinstance(policy, DDP) else policy
            policy_to_push.push_model_to_hub(cfg)
            preprocessor.push_to_hub(cfg.policy.repo_id)
            postprocessor.push_to_hub(cfg.policy.repo_id)

    if world_size > 1:
        dist.barrier()

    cleanup_ddp()


def main():
    train()


if __name__ == "__main__":
    main()
