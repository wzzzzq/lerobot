#!/usr/bin/env python

# Copyright 2025 HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Reflow training script for SmolVLA.

This script extends the standard lerobot_train.py to support Reflow (Rectified Flow) training.

Key design principles:
1. SmolVLA model code remains COMPLETELY CLEAN - no reflow-specific code
2. Reflow logic is ONLY in the training script
3. Teacher model is instantiated HERE, not in the model class
4. X_1 (target actions) are generated by teacher.sample_actions()
5. X_1 is passed as the 'actions' parameter to student.forward()

This follows the principle: "Reflow is a training method, not a model architecture"

Important:
- Both teacher and student load from the SAME checkpoint path
- VLM and vision encoder are ALWAYS frozen (only trains expert/action head)
- Student starts with teacher's weights, then fine-tuned with straight-line targets
- Student's forward() uses standard Flow Matching loss, but with X_1 as target

Usage:
    python lerobot_train_reflow.py \\
        --policy.type=smolvla \\
        --policy.teacher_model_path=/path/to/teacher \\
        --policy.optimizer_lr=2e-5 \\
        --dataset.repo_id=your_dataset \\
        --steps=20000
"""

import logging
import sys
import os
from dataclasses import dataclass
from pathlib import Path

# Add the scripts directory to the path so we can import from lerobot_train
sys.path.insert(0, os.path.dirname(__file__))
from lerobot_train import *  # noqa: F403, E402

import torch
from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy, pad_vector
from lerobot.configs.train import TrainPipelineConfig
from lerobot.rl.wandb_utils import WandBLogger


@dataclass
class ReflowTrainPipelineConfig(TrainPipelineConfig):
    """Extended training configuration for Reflow training.
    
    Adds teacher_model_path as a top-level argument, keeping it separate
    from policy configuration for clean architecture.
    """
    # Path to the pretrained teacher model checkpoint
    teacher_model_path: Path | None = None

    def validate(self) -> None:
        """Validate config and set up optimizer/scheduler from policy presets."""
        # Call parent validation to set up optimizer/scheduler from presets
        super().validate()
        
        # Validate teacher_model_path for reflow training
        if self.teacher_model_path is None:
            raise ValueError(
                "teacher_model_path is required for reflow training. "
                "Please specify --teacher_model_path=/path/to/teacher/checkpoint"
            )
        
        if not Path(self.teacher_model_path).exists():
            raise FileNotFoundError(
                f"Teacher model path does not exist: {self.teacher_model_path}"
            )


def setup_reflow_models(cfg, ds_meta):
    """Setup teacher and student models for reflow training.

    In reflow training:
    - Teacher: Frozen model that generates straight-line targets (X_1)
    - Student: Initialized from teacher, learns to predict straight-line flows

    This function loads BOTH models and freezes the appropriate components.

    Args:
        cfg: ReflowTrainPipelineConfig with teacher_model_path
        ds_meta: Dataset metadata

    Returns:
        tuple: (teacher_policy, student_policy)
            - teacher_policy: Frozen teacher model for generating X_1
            - student_policy: Student model to be trained
    """
    if cfg.policy.type != "smolvla":
        raise ValueError("Reflow training is only supported for smolvla policy")

    if not cfg.teacher_model_path:
        raise ValueError(
            "teacher_model_path must be specified for reflow training. "
            "Use --teacher_model_path=/path/to/teacher"
        )

    logging.info(f"[Reflow] Loading teacher model from {cfg.teacher_model_path}")

    # Load teacher model (will be frozen for generating X_1)
    teacher = SmolVLAPolicy.from_pretrained(cfg.teacher_model_path)
    teacher.eval()

    # Freeze ALL teacher parameters
    for param in teacher.parameters():
        param.requires_grad = False

    logging.info("[Reflow] ✓ Teacher model loaded and frozen")

    # Initialize student from the same checkpoint
    # This is key for reflow: student starts with teacher's weights
    logging.info(f"[Reflow] Initializing student model from {cfg.teacher_model_path}")
    student = SmolVLAPolicy.from_pretrained(cfg.teacher_model_path)

    # Freeze VLM and vision encoder in student
    # Reflow only fine-tunes the expert (action head)
    logging.info("[Reflow] Freezing vision encoder and language model (reflow only trains expert)")

    # Freeze vision model
    student.model.vlm_with_expert.get_vlm_model().vision_model.eval()
    for param in student.model.vlm_with_expert.get_vlm_model().vision_model.parameters():
        param.requires_grad = False

    # Freeze language model (VLM)
    student.model.vlm_with_expert.vlm.eval()
    for param in student.model.vlm_with_expert.vlm.parameters():
        param.requires_grad = False

    logging.info("[Reflow] ✓ Student initialized from teacher checkpoint")

    # Log parameter counts
    total_params = sum(p.numel() for p in student.parameters())
    trainable_params = sum(p.numel() for p in student.parameters() if p.requires_grad)
    logging.info(f"[Reflow] Total parameters: {total_params:,}")
    logging.info(f"[Reflow] Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")

    return teacher, student


def prepare_reflow_batch(teacher, student, batch):
    """Prepare batch for reflow training by generating teacher targets.

    Key insight: Reflow uses the SAME time parameterization as standard FM.

    Standard FM:
        - t=0: data, t=1: noise
        - x_t = t*noise + (1-t)*actions
        - u_t = noise - actions
        - Inference: from t=1 (noise) backward to t=0 (data)

    Reflow:
        - t=0: data (teacher-generated), t=1: noise
        - x_t = t*noise + (1-t)*actions
        - u_t = noise - actions (straight line!)
        - Inference: from t=1 (noise) backward to t=0 (data), same as FM!

    Solution:
        - X_1 = random noise (t=1)
        - X_0 = teacher.model.sample_actions(noise=X_1) (t=0, integrated from noise)
        - Preprocess observations ONCE (no duplicate prepare calls)
        - Return everything ready for model.forward()

    Args:
        teacher: Frozen teacher policy
        student: Student policy (for prepare methods)
        batch: Training batch with observations

    Returns:
        tuple: (images, img_masks, lang_tokens, lang_masks, state, X_0, X_1, actions_is_pad)
            - All inputs preprocessed and ready for model.forward()
            - X_0: teacher-generated actions (t=0)
            - X_1: random noise (t=1)
            - actions_is_pad: padding mask for loss computation
    """
    from lerobot.utils.constants import OBS_LANGUAGE_TOKENS, OBS_LANGUAGE_ATTENTION_MASK, OBS_STATE, ACTION
    
    device = batch["observation.state"].device
    dtype = batch["action"].dtype

    # Handle pi_aloha adaptation if needed (same as policy.forward)
    if student.config.adapt_to_pi_aloha:
        batch[OBS_STATE] = student._pi_aloha_decode_state(batch[OBS_STATE])
        batch[ACTION] = student._pi_aloha_encode_actions_inv(batch[ACTION])

    # Preprocess observations ONCE (shared by teacher and student)
    images, img_masks = student.prepare_images(batch)
    state = student.prepare_state(batch)
    lang_tokens = batch[f"{OBS_LANGUAGE_TOKENS}"]
    lang_masks = batch[f"{OBS_LANGUAGE_ATTENTION_MASK}"]
    actions_is_pad = batch.get("actions_is_pad")

    # Sample X_1 (random noise at t=1)
    action_shape = batch["action"].shape
    X_1 = torch.randn(action_shape, device=device, dtype=dtype)

    # Pad X_1 to max_action_dim for teacher model
    # The teacher's action_in_proj expects padded actions
    X_1_padded = pad_vector(X_1, teacher.config.max_action_dim)

    # Generate X_0 using teacher's ODE: integrate from t=1 (X_1) to t=0 (X_0)
    # Use the SAME preprocessed inputs (no duplicate preprocessing!)
    with torch.no_grad():
        teacher.eval()
        X_0 = teacher.model.sample_actions(
            images, img_masks, lang_tokens, lang_masks, state, noise=X_1_padded
        )
        # CRITICAL FIX: Do NOT unpad X_0!
        # Teacher's X_0 may have non-zero values in padding dimensions due to ODE integration.
        # Unpading would lose this information and create train/inference mismatch.
        # Keep X_0 at full max_action_dim (32) to preserve teacher's actual output.

    # Return preprocessed inputs and X_0, X_1
    # NOTE: X_0 is now max_action_dim (32), not original action_dim (14)
    return images, img_masks, lang_tokens, lang_masks, state, X_0, X_1, actions_is_pad


@parser.wrap()  # noqa: F405
def main(cfg: ReflowTrainPipelineConfig):  # noqa: F405
    """Main training function with reflow support."""
    # Validate config and set up optimizer/scheduler from presets
    cfg.validate()
    
    init_logging()  # noqa: F405

    # Log config
    logging.info(f"Training with config:\n{pformat(cfg)}")  # noqa: F405

    # Make dataset
    logging.info("Making dataset...")
    dataset = make_dataset(cfg)  # noqa: F405

    # Setup teacher and student models
    # NOTE: Both load from teacher_model_path - student starts from teacher's weights
    logging.info("Setting up teacher and student models...")
    teacher, student = setup_reflow_models(cfg, dataset.meta)

    # Use student as the policy to train
    policy = student
    pre_processor, post_processor = make_pre_post_processors(cfg.policy)  # noqa: F405

    # Make optimizer and scheduler
    logging.info("Making optimizer and scheduler...")
    optimizer, lr_scheduler = make_optimizer_and_scheduler(cfg, policy)  # noqa: F405

    # Create dataloader
    logging.info("Creating dataloader...")
    if hasattr(cfg.policy, "drop_n_last_frames"):
        shuffle = False
        sampler = EpisodeAwareSampler(  # noqa: F405
            dataset.meta.episodes["dataset_from_index"],
            dataset.meta.episodes["dataset_to_index"],
            drop_n_last_frames=cfg.policy.drop_n_last_frames,
            shuffle=True,
        )
    else:
        shuffle = True
        sampler = None

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=cfg.num_workers,
        batch_size=cfg.batch_size,
        shuffle=shuffle and not cfg.dataset.streaming,
        sampler=sampler,
        pin_memory=device.type == "cuda",
        drop_last=False,
        prefetch_factor=2 if cfg.num_workers > 0 else None,
    )

    # Custom training loop for reflow
    logging.info("Starting reflow training loop...")

    step = 0
    data_iter = cycle(dataloader)  # noqa: F405

    # Move models to device
    teacher = teacher.to(device)
    policy = policy.to(device)

    # Initialize wandb logger
    if cfg.wandb.enable and cfg.wandb.project:
        wandb_logger = WandBLogger(cfg)
        logging.info("✓ WandB logging enabled")
    else:
        wandb_logger = None
        logging.info("WandB logging disabled")

    logging.info(f"Using device: {device}")
    logging.info("=" * 80)
    logging.info("Starting Reflow Training")
    logging.info("=" * 80)

    while step < cfg.steps:
        # Get next batch
        batch = next(data_iter)

        # Preprocess batch (includes device transfer via DeviceProcessorStep)
        batch = pre_processor(batch)

        # === REFLOW CORE: Prepare reflow batch ===
        # Preprocess observations once and generate teacher actions:
        # 1. Preprocess observations (images, state, etc.) - done ONCE
        # 2. Sample X_1 (random noise at t=1)
        # 3. Generate X_0 via teacher ODE (integrate from t=1 to t=0)
        # 4. Return preprocessed inputs + X_0, X_1 for model.forward()
        # This avoids duplicate preprocessing in prepare_images/prepare_state
        images, img_masks, lang_tokens, lang_masks, state, X_0, X_1, actions_is_pad = prepare_reflow_batch(
            teacher, policy, batch
        )

        # CRITICAL FIX: X_0 is already at max_action_dim (32) from teacher.sample_actions()
        # Do NOT pad again! This would create a double-pad bug.
        # Only pad X_1 since it's sampled at original action_dim (14)
        X_1_padded = pad_vector(X_1, policy.config.max_action_dim)  # noqa: F405

        # Forward pass directly on model (no duplicate preprocessing!)
        # X_0 is used directly (already 32-dim), X_1_padded is padded to 32-dim
        policy.train()
        losses = policy.model.forward(
            images, img_masks, lang_tokens, lang_masks, state, X_0, noise=X_1_padded, time=None
        )

        # Apply action padding mask (same as policy.forward)
        if actions_is_pad is not None:
            in_episode_bound = ~actions_is_pad
            losses = losses * in_episode_bound.unsqueeze(-1)

        # Remove padding dimension (same as policy.forward)
        losses = losses[:, :, : policy.config.action_feature.shape[0]]

        # Compute scalar loss
        loss = losses.mean()

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()

        # Clip gradients
        if cfg.optimizer.grad_clip_norm > 0:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                policy.parameters(), cfg.optimizer.grad_clip_norm
            )
        else:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                policy.parameters(), float("inf"), error_if_nonfinite=False
            )

        optimizer.step()

        if lr_scheduler is not None:
            lr_scheduler.step()

        step += 1

        # Logging
        if cfg.log_freq > 0 and step % cfg.log_freq == 0:
            current_lr = optimizer.param_groups[0]['lr']
            logging.info(
                f"Step {step}/{cfg.steps} | "
                f"Loss: {loss.item():.4f} | "
                f"Grad Norm: {grad_norm.item():.4f} | "
                f"LR: {current_lr:.2e}"
            )

            # WandB logging
            if wandb_logger:
                # Compute detailed loss statistics for logging
                with torch.no_grad():
                    loss_std = losses.std().item()
                    loss_max = losses.max().item()
                    loss_min = losses.min().item()
                    # Per-dimension loss (averaged over batch and chunk)
                    loss_per_dim = losses.mean(dim=[0, 1]).cpu().numpy()  # (action_dim,)

                wandb_log_dict = {
                    "train/loss": loss.item(),
                    "train/loss_std": loss_std,
                    "train/loss_max": loss_max,
                    "train/loss_min": loss_min,
                    "train/grad_norm": grad_norm.item(),
                    "train/learning_rate": current_lr,
                }
                # Add per-dimension loss (only first 14 dims to keep wandb clean)
                for i in range(min(14, len(loss_per_dim))):
                    wandb_log_dict[f"train/loss_dim_{i}"] = loss_per_dim[i]

                wandb_logger.log_dict(wandb_log_dict, step)

        # Save checkpoint
        if step % cfg.save_freq == 0 or step == cfg.steps:
            logging.info(f"Saving checkpoint at step {step}...")
            checkpoint_dir = get_step_checkpoint_dir(cfg.output_dir, cfg.steps, step)  # noqa: F405
            save_checkpoint(  # noqa: F405
                checkpoint_dir=checkpoint_dir,
                step=step,
                cfg=cfg,
                policy=policy,
                optimizer=optimizer,
                scheduler=lr_scheduler,
                preprocessor=pre_processor,
                postprocessor=post_processor,
            )
            update_last_checkpoint(checkpoint_dir)  # noqa: F405
            logging.info(f"✓ Checkpoint saved to {checkpoint_dir}")

            # Log checkpoint to wandb
            if wandb_logger:
                wandb_logger.log_policy(checkpoint_dir)

        # Evaluation (if enabled)
        if cfg.eval_freq > 0 and step % cfg.eval_freq == 0 and cfg.env is not None:
            logging.info(f"Evaluation at step {step} (skipped - evaluation not implemented in reflow script)")

    logging.info("=" * 80)
    logging.info("Reflow Training Completed!")
    logging.info("=" * 80)


if __name__ == "__main__":
    main()
