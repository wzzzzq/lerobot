#!/usr/bin/env python

# Copyright 2025 HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Reflow training script for SmolVLA.

This script extends the standard lerobot_train.py to support Reflow (Rectified Flow) training.

Key design principles:
1. SmolVLA model code remains COMPLETELY CLEAN - no reflow-specific code
2. Reflow logic is ONLY in the training script
3. Teacher model is instantiated HERE, not in the model class
4. X_1 (target actions) are generated by teacher.sample_actions()
5. X_1 is passed as the 'actions' parameter to student.forward()

This follows the principle: "Reflow is a training method, not a model architecture"

Important:
- Both teacher and student load from the SAME checkpoint path
- VLM and vision encoder are ALWAYS frozen (only trains expert/action head)
- Student starts with teacher's weights, then fine-tuned with straight-line targets
- Student's forward() uses standard Flow Matching loss, but with X_1 as target

Usage:
    python lerobot_train_reflow.py \\
        --policy.type=smolvla \\
        --policy.teacher_model_path=/path/to/teacher \\
        --policy.optimizer_lr=2e-5 \\
        --dataset.repo_id=your_dataset \\
        --steps=20000
"""

import logging
import sys
import os

# Add the scripts directory to the path so we can import from lerobot_train
sys.path.insert(0, os.path.dirname(__file__))
from lerobot_train import *  # noqa: F403, E402

import torch
from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy, pad_vector


def setup_reflow_models(cfg, ds_meta):
    """Setup teacher and student models for reflow training.

    In reflow training:
    - Teacher: Frozen model that generates straight-line targets (X_1)
    - Student: Initialized from teacher, learns to predict straight-line flows

    This function loads BOTH models and freezes the appropriate components.

    Args:
        cfg: Training configuration
        ds_meta: Dataset metadata

    Returns:
        tuple: (teacher_policy, student_policy)
            - teacher_policy: Frozen teacher model for generating X_1
            - student_policy: Student model to be trained
    """
    if cfg.policy.type != "smolvla":
        raise ValueError("Reflow training is only supported for smolvla policy")

    teacher_path = getattr(cfg.policy, "teacher_model_path", None)
    if not teacher_path:
        raise ValueError(
            "teacher_model_path must be specified for reflow training. "
            "Use --policy.teacher_model_path=/path/to/teacher"
        )

    logging.info(f"[Reflow] Loading teacher model from {teacher_path}")

    # Load teacher model (will be frozen for generating X_1)
    teacher = SmolVLAPolicy.from_pretrained(teacher_path)
    teacher.eval()

    # Freeze ALL teacher parameters
    for param in teacher.parameters():
        param.requires_grad = False

    logging.info("[Reflow] ✓ Teacher model loaded and frozen")

    # Initialize student from the same checkpoint
    # This is key for reflow: student starts with teacher's weights
    logging.info(f"[Reflow] Initializing student model from {teacher_path}")
    student = SmolVLAPolicy.from_pretrained(teacher_path)

    # Freeze VLM and vision encoder in student
    # Reflow only fine-tunes the expert (action head)
    logging.info("[Reflow] Freezing vision encoder and language model (reflow only trains expert)")

    # Freeze vision model
    student.model.vlm_with_expert.get_vlm_model().vision_model.eval()
    for param in student.model.vlm_with_expert.get_vlm_model().vision_model.parameters():
        param.requires_grad = False

    # Freeze language model (VLM)
    student.model.vlm_with_expert.vlm.eval()
    for param in student.model.vlm_with_expert.vlm.parameters():
        param.requires_grad = False

    logging.info("[Reflow] ✓ Student initialized from teacher checkpoint")

    # Log parameter counts
    total_params = sum(p.numel() for p in student.parameters())
    trainable_params = sum(p.numel() for p in student.parameters() if p.requires_grad)
    logging.info(f"[Reflow] Total parameters: {total_params:,}")
    logging.info(f"[Reflow] Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")

    return teacher, student


def prepare_reflow_batch(teacher, student, batch):
    """Prepare batch for reflow training by generating teacher targets.

    Key insight: Reflow uses the SAME time parameterization as standard FM.

    Standard FM:
        - t=0: data, t=1: noise
        - x_t = t*noise + (1-t)*actions
        - u_t = noise - actions
        - Inference: from t=1 (noise) backward to t=0 (data)

    Reflow:
        - t=0: data (teacher-generated), t=1: noise
        - x_t = t*noise + (1-t)*actions
        - u_t = noise - actions (straight line!)
        - Inference: from t=1 (noise) backward to t=0 (data), same as FM!

    Solution:
        - X_1 = random noise (t=1)
        - X_0 = teacher.model.sample_actions(noise=X_1) (t=0, integrated from noise)
        - Preprocess observations ONCE (no duplicate prepare calls)
        - Return everything ready for model.forward()

    Args:
        teacher: Frozen teacher policy
        student: Student policy (for prepare methods)
        batch: Training batch with observations

    Returns:
        tuple: (images, img_masks, lang_tokens, lang_masks, state, X_0, X_1, actions_is_pad)
            - All inputs preprocessed and ready for model.forward()
            - X_0: teacher-generated actions (t=0)
            - X_1: random noise (t=1)
            - actions_is_pad: padding mask for loss computation
    """
    device = batch["observation.state"].device
    dtype = batch["action"].dtype

    # Handle pi_aloha adaptation if needed (same as policy.forward)
    if student.config.adapt_to_pi_aloha:
        from lerobot.common.constants import OBS_STATE, ACTION
        batch[OBS_STATE] = student._pi_aloha_decode_state(batch[OBS_STATE])
        batch[ACTION] = student._pi_aloha_encode_actions_inv(batch[ACTION])

    # Preprocess observations ONCE (shared by teacher and student)
    images, img_masks = student.prepare_images(batch)
    state = student.prepare_state(batch)
    lang_tokens = batch["observation.environment_language_tokens"]
    lang_masks = batch["observation.environment_language_attention_mask"]
    actions_is_pad = batch.get("actions_is_pad")

    # Sample X_1 (random noise at t=1)
    action_shape = batch["action"].shape
    X_1 = torch.randn(action_shape, device=device, dtype=dtype)

    # Generate X_0 using teacher's ODE: integrate from t=1 (X_1) to t=0 (X_0)
    # Use the SAME preprocessed inputs (no duplicate preprocessing!)
    with torch.no_grad():
        teacher.eval()
        X_0 = teacher.model.sample_actions(
            images, img_masks, lang_tokens, lang_masks, state, noise=X_1
        )

        # Unpad actions to match original action dimension
        original_action_dim = teacher.config.action_feature.shape[0]
        X_0 = X_0[:, :, :original_action_dim]

    # Return preprocessed inputs and X_0, X_1
    return images, img_masks, lang_tokens, lang_masks, state, X_0, X_1, actions_is_pad


@parser.wrap()  # noqa: F405
def main(cfg):  # noqa: F405
    """Main training function with reflow support."""
    init_logging()  # noqa: F405

    # Log config
    logging.info(f"Training with config:\n{pformat(cfg)}")  # noqa: F405

    # Make dataset
    logging.info("Making dataset...")
    ds_meta, train_dataloader = make_dataset(cfg)  # noqa: F405

    # Setup teacher and student models
    # NOTE: Both load from teacher_model_path - student starts from teacher's weights
    logging.info("Setting up teacher and student models...")
    teacher, student = setup_reflow_models(cfg, ds_meta)

    # Use student as the policy to train
    policy = student
    pre_processor, post_processor = make_pre_post_processors(cfg.policy)  # noqa: F405

    # Make optimizer and scheduler
    logging.info("Making optimizer and scheduler...")
    optimizer, lr_scheduler = make_optimizer_and_scheduler(cfg.policy, policy)  # noqa: F405

    # Make environment (optional)
    env = None
    eval_batch_size = None
    if cfg.evaluate.enabled and cfg.evaluate.on_real_robot:
        logging.info("Making environment...")
        env, eval_batch_size = make_env_eval_online(cfg)  # noqa: F405

    # Custom training loop for reflow
    logging.info("Starting reflow training loop...")

    step = 0
    data_iter = cycle(train_dataloader)  # noqa: F405

    # Move models to device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    teacher = teacher.to(device)
    policy = policy.to(device)

    logging.info(f"Using device: {device}")
    logging.info("=" * 80)
    logging.info("Starting Reflow Training")
    logging.info("=" * 80)

    while step < cfg.steps:
        # Get next batch
        batch = next(data_iter)

        # Preprocess batch (includes device transfer via DeviceProcessorStep)
        batch = pre_processor(batch)

        # === REFLOW CORE: Prepare reflow batch ===
        # Preprocess observations once and generate teacher actions:
        # 1. Preprocess observations (images, state, etc.) - done ONCE
        # 2. Sample X_1 (random noise at t=1)
        # 3. Generate X_0 via teacher ODE (integrate from t=1 to t=0)
        # 4. Return preprocessed inputs + X_0, X_1 for model.forward()
        # This avoids duplicate preprocessing in prepare_images/prepare_state
        images, img_masks, lang_tokens, lang_masks, state, X_0, X_1, actions_is_pad = prepare_reflow_batch(
            teacher, policy, batch
        )

        # Pad X_0 to max_action_dim for model.forward
        X_0_padded = pad_vector(X_0, policy.config.max_action_dim)  # noqa: F405

        # Forward pass directly on model (no duplicate preprocessing!)
        policy.train()
        losses = policy.model.forward(
            images, img_masks, lang_tokens, lang_masks, state, X_0_padded, noise=X_1, time=None
        )

        # Apply action padding mask (same as policy.forward)
        if actions_is_pad is not None:
            in_episode_bound = ~actions_is_pad
            losses = losses * in_episode_bound.unsqueeze(-1)

        # Remove padding dimension (same as policy.forward)
        losses = losses[:, :, : policy.config.action_feature.shape[0]]

        # Compute scalar loss
        loss = losses.mean()

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()

        # Clip gradients
        if cfg.optimizer.grad_clip_norm > 0:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                policy.parameters(), cfg.optimizer.grad_clip_norm
            )
        else:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                policy.parameters(), float("inf"), error_if_nonfinite=False
            )

        optimizer.step()

        if lr_scheduler is not None:
            lr_scheduler.step()

        step += 1

        # Logging
        if cfg.log_freq > 0 and step % cfg.log_freq == 0:
            logging.info(
                f"Step {step}/{cfg.steps} | "
                f"Loss: {loss.item():.4f} | "
                f"Grad Norm: {grad_norm.item():.4f} | "
                f"LR: {optimizer.param_groups[0]['lr']:.2e}"
            )

        # Save checkpoint
        if step % cfg.save_freq == 0 or step == cfg.steps:
            logging.info(f"Saving checkpoint at step {step}...")
            save_path = os.path.join(cfg.output_dir, f"checkpoint_{step}")
            policy.save_pretrained(save_path)
            logging.info(f"✓ Checkpoint saved to {save_path}")

        # Evaluation (if enabled)
        if cfg.evaluate.enabled and step % cfg.evaluate.freq == 0:
            logging.info(f"Running evaluation at step {step}...")
            if env is not None:
                # Online evaluation on real robot
                eval_info = evaluate_on_env(  # noqa: F405
                    env,
                    policy,
                    cfg.evaluate.num_episodes,
                    eval_batch_size,
                    device,
                )
                logging.info(f"Evaluation results: {eval_info}")

    logging.info("=" * 80)
    logging.info("Reflow Training Completed!")
    logging.info("=" * 80)


if __name__ == "__main__":
    main()
