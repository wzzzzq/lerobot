#!/usr/bin/env python

# Copyright 2025 HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Reflow training script for SmolVLA.

This script extends the standard lerobot_train.py to support Reflow (Rectified Flow) training.

Key design principles:
1. SmolVLA model code remains COMPLETELY CLEAN - no reflow-specific code
2. Reflow logic is ONLY in the training script
3. Teacher model is instantiated HERE, not in the model class
4. X_1 (target actions) are generated by teacher.sample_actions()
5. X_1 is passed as the 'actions' parameter to student.forward()

This follows the principle: "Reflow is a training method, not a model architecture"

Important:
- Both teacher and student load from the SAME checkpoint path
- VLM and vision encoder are ALWAYS frozen (only trains expert/action head)
- Student starts with teacher's weights, then fine-tuned with straight-line targets
- Student's forward() uses standard Flow Matching loss, but with X_1 as target

Usage:
    python lerobot_train_reflow.py \\
        --policy.type=smolvla \\
        --policy.teacher_model_path=/path/to/teacher \\
        --policy.optimizer_lr=2e-5 \\
        --dataset.repo_id=your_dataset \\
        --steps=20000
"""

import logging
import sys
import os

# Add the scripts directory to the path so we can import from lerobot_train
sys.path.insert(0, os.path.dirname(__file__))
from lerobot_train import *  # noqa: F403, E402

import torch
from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy


def setup_reflow_models(cfg, ds_meta):
    """Setup teacher and student models for reflow training.

    In reflow training:
    - Teacher: Frozen model that generates straight-line targets (X_1)
    - Student: Initialized from teacher, learns to predict straight-line flows

    This function loads BOTH models and freezes the appropriate components.

    Args:
        cfg: Training configuration
        ds_meta: Dataset metadata

    Returns:
        tuple: (teacher_policy, student_policy)
            - teacher_policy: Frozen teacher model for generating X_1
            - student_policy: Student model to be trained
    """
    if cfg.policy.type != "smolvla":
        raise ValueError("Reflow training is only supported for smolvla policy")

    teacher_path = getattr(cfg.policy, "teacher_model_path", None)
    if not teacher_path:
        raise ValueError(
            "teacher_model_path must be specified for reflow training. "
            "Use --policy.teacher_model_path=/path/to/teacher"
        )

    logging.info(f"[Reflow] Loading teacher model from {teacher_path}")

    # Load teacher model (will be frozen for generating X_1)
    teacher = SmolVLAPolicy.from_pretrained(teacher_path)
    teacher.eval()

    # Freeze ALL teacher parameters
    for param in teacher.parameters():
        param.requires_grad = False

    logging.info("[Reflow] ✓ Teacher model loaded and frozen")

    # Initialize student from the same checkpoint
    # This is key for reflow: student starts with teacher's weights
    logging.info(f"[Reflow] Initializing student model from {teacher_path}")
    student = SmolVLAPolicy.from_pretrained(teacher_path)

    # Freeze VLM and vision encoder in student
    # Reflow only fine-tunes the expert (action head)
    logging.info("[Reflow] Freezing vision encoder and language model (reflow only trains expert)")

    # Freeze vision model
    student.model.vlm_with_expert.get_vlm_model().vision_model.eval()
    for param in student.model.vlm_with_expert.get_vlm_model().vision_model.parameters():
        param.requires_grad = False

    # Freeze language model (VLM)
    student.model.vlm_with_expert.vlm.eval()
    for param in student.model.vlm_with_expert.vlm.parameters():
        param.requires_grad = False

    logging.info("[Reflow] ✓ Student initialized from teacher checkpoint")

    # Log parameter counts
    total_params = sum(p.numel() for p in student.parameters())
    trainable_params = sum(p.numel() for p in student.parameters() if p.requires_grad)
    logging.info(f"[Reflow] Total parameters: {total_params:,}")
    logging.info(f"[Reflow] Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")

    return teacher, student


def prepare_reflow_batch(teacher, batch):
    """Prepare batch for reflow training by generating teacher targets.

    Key insight: Reflow uses the SAME time parameterization as standard FM.

    Standard FM:
        - t=0: data, t=1: noise
        - x_t = t*noise + (1-t)*actions
        - u_t = noise - actions
        - Inference: from t=1 (noise) backward to t=0 (data)

    Reflow:
        - t=0: data (teacher-generated), t=1: noise
        - x_t = t*noise + (1-t)*actions
        - u_t = noise - actions (straight line!)
        - Inference: from t=1 (noise) backward to t=0 (data), same as FM!

    Solution:
        - X_1 = random noise (t=1)
        - X_0 = teacher.model.sample_actions(noise=X_1) (t=0, integrated from noise)
        - Set actions=X_0, noise=X_1
        - forward() computes: x_t = t*X_1 + (1-t)*X_0, u_t = X_1 - X_0
        - This matches FM exactly, just with teacher-generated data!

    Args:
        teacher: Frozen teacher policy
        batch: Training batch with observations

    Returns:
        tuple: (modified_batch, X_1_noise) - batch with actions=X_0, and X_1 to pass as noise
    """
    device = batch["observation.state"].device
    dtype = batch["action"].dtype

    # Sample X_1 (random noise at t=1)
    action_shape = batch["action"].shape
    X_1 = torch.randn(action_shape, device=device, dtype=dtype)

    # Generate X_0 using teacher's ODE: integrate from t=1 (X_1) to t=0 (X_0)
    # This is the same as standard inference: noise -> data
    with torch.no_grad():
        teacher.eval()
        # Prepare inputs for teacher model
        images, img_masks = teacher.prepare_images(batch)
        state = teacher.prepare_state(batch)
        lang_tokens = batch["observation.environment_language_tokens"]
        lang_masks = batch["observation.environment_language_attention_mask"]

        # Call model.sample_actions directly (no queue management overhead)
        X_0 = teacher.model.sample_actions(
            images, img_masks, lang_tokens, lang_masks, state, noise=X_1
        )

        # Unpad actions to match original action dimension
        original_action_dim = teacher.config.action_feature.shape[0]
        X_0 = X_0[:, :, :original_action_dim]

    # Modify batch: set actions to X_0 (teacher-generated data at t=0)
    modified_batch = batch.copy()
    modified_batch["action"] = X_0

    # Return X_1 (noise at t=1) to be passed as "noise" parameter to forward()
    return modified_batch, X_1


def main():
    """Main training function with reflow support."""
    # Parse config
    cfg = parser.parse_args_to_cfg()  # noqa: F405
    init_logging()  # noqa: F405

    # Log config
    logging.info(f"Training with config:\n{pformat(cfg)}")  # noqa: F405

    # Make dataset
    logging.info("Making dataset...")
    ds_meta, train_dataloader = make_dataset(cfg)  # noqa: F405

    # Setup teacher and student models
    # NOTE: Both load from teacher_model_path - student starts from teacher's weights
    logging.info("Setting up teacher and student models...")
    teacher, student = setup_reflow_models(cfg, ds_meta)

    # Use student as the policy to train
    policy = student
    pre_processor, post_processor = make_pre_post_processors(cfg.policy)  # noqa: F405

    # Make optimizer and scheduler
    logging.info("Making optimizer and scheduler...")
    optimizer, lr_scheduler = make_optimizer_and_scheduler(cfg.policy, policy)  # noqa: F405

    # Make environment (optional)
    env = None
    eval_batch_size = None
    if cfg.evaluate.enabled and cfg.evaluate.on_real_robot:
        logging.info("Making environment...")
        env, eval_batch_size = make_env_eval_online(cfg)  # noqa: F405

    # Custom training loop for reflow
    logging.info("Starting reflow training loop...")

    step = 0
    data_iter = cycle(train_dataloader)  # noqa: F405

    # Move models to device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    teacher = teacher.to(device)
    policy = policy.to(device)

    logging.info(f"Using device: {device}")
    logging.info("=" * 80)
    logging.info("Starting Reflow Training")
    logging.info("=" * 80)

    while step < cfg.steps:
        # Get next batch
        batch = next(data_iter)

        # Preprocess batch
        batch = pre_processor(batch)
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}

        # === REFLOW CORE: Prepare reflow batch ===
        # This cleverly reuses SmolVLA's forward() by passing teacher-generated data:
        # 1. Sample X_1 (random noise at t=1)
        # 2. Generate X_0 via teacher ODE (integrate from t=1 to t=0)
        # 3. Set batch["action"] = X_0 (teacher-generated data)
        # 4. Pass X_1 as noise to forward()
        # Then forward() computes x_t = t*X_1 + (1-t)*X_0, u_t = X_1 - X_0 ✓
        modified_batch, X_1_noise = prepare_reflow_batch(teacher, batch)

        # Forward with modified batch
        # SmolVLA doesn't know this is reflow - it just computes its normal loss!
        policy.train()
        loss, output_dict = policy.forward(modified_batch, noise=X_1_noise)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()

        # Clip gradients
        if cfg.optimizer.grad_clip_norm > 0:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                policy.parameters(), cfg.optimizer.grad_clip_norm
            )
        else:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                policy.parameters(), float("inf"), error_if_nonfinite=False
            )

        optimizer.step()

        if lr_scheduler is not None:
            lr_scheduler.step()

        step += 1

        # Logging
        if cfg.log_freq > 0 and step % cfg.log_freq == 0:
            logging.info(
                f"Step {step}/{cfg.steps} | "
                f"Loss: {loss.item():.4f} | "
                f"Grad Norm: {grad_norm.item():.4f} | "
                f"LR: {optimizer.param_groups[0]['lr']:.2e}"
            )

        # Save checkpoint
        if step % cfg.save_freq == 0 or step == cfg.steps:
            logging.info(f"Saving checkpoint at step {step}...")
            save_path = os.path.join(cfg.output_dir, f"checkpoint_{step}")
            policy.save_pretrained(save_path)
            logging.info(f"✓ Checkpoint saved to {save_path}")

        # Evaluation (if enabled)
        if cfg.evaluate.enabled and step % cfg.evaluate.freq == 0:
            logging.info(f"Running evaluation at step {step}...")
            if env is not None:
                # Online evaluation on real robot
                eval_info = evaluate_on_env(  # noqa: F405
                    env,
                    policy,
                    cfg.evaluate.num_episodes,
                    eval_batch_size,
                    device,
                )
                logging.info(f"Evaluation results: {eval_info}")

    logging.info("=" * 80)
    logging.info("Reflow Training Completed!")
    logging.info("=" * 80)


if __name__ == "__main__":
    main()
