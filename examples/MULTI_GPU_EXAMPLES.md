# Multi-GPU Training Examples

æœ¬ç›®å½•åŒ…å«å¤šå¡è®­ç»ƒçš„ç¤ºä¾‹è„šæœ¬ã€‚

## ğŸ“ è„šæœ¬è¯´æ˜

### 1. `train_smolvla_multi_gpu.sh`

é’ˆå¯¹ SmolVLA æ¨¡å‹çš„å¤šå¡è®­ç»ƒç¤ºä¾‹ï¼ŒåŒ…å«å®Œæ•´çš„é…ç½®ã€‚

**ç‰¹ç‚¹**ï¼š
- é¢„é…ç½®çš„ SmolVLA è®­ç»ƒå‚æ•°
- 4 GPU é…ç½®
- BF16 æ··åˆç²¾åº¦
- WandB æ—¥å¿—è®°å½•

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
# ä¿®æ”¹è„šæœ¬ä¸­çš„é…ç½®ï¼ˆæ•°æ®é›†è·¯å¾„ã€è¾“å‡ºç›®å½•ç­‰ï¼‰
vim examples/train_smolvla_multi_gpu.sh

# è¿è¡Œè®­ç»ƒ
bash examples/train_smolvla_multi_gpu.sh
```

### 2. `train_multi_gpu_template.sh`

é€šç”¨å¤šå¡è®­ç»ƒæ¨¡æ¿ï¼Œå¯ç”¨äºä»»ä½•ç­–ç•¥å’Œæ•°æ®é›†ã€‚

**ç‰¹ç‚¹**ï¼š
- è¯¦ç»†çš„é…ç½®æ³¨é‡Š
- çµæ´»çš„å‚æ•°é…ç½®
- é€‚ç”¨äºæ‰€æœ‰ç­–ç•¥ç±»å‹
- åŒ…å«é”™è¯¯æ£€æŸ¥å’Œé…ç½®æ˜¾ç¤º

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
# 1. å¤åˆ¶æ¨¡æ¿
cp examples/train_multi_gpu_template.sh examples/my_training.sh

# 2. ä¿®æ”¹é…ç½®
vim examples/my_training.sh
# ä¸»è¦ä¿®æ”¹ï¼š
#   - POLICY_PATH: ç­–ç•¥ç±»å‹æˆ–é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
#   - DATASET_REPO_ID: ä½ çš„æ•°æ®é›† ID
#   - NUM_GPUS: GPU æ•°é‡
#   - BATCH_SIZE: æ¯ä¸ª GPU çš„ batch size

# 3. è¿è¡Œè®­ç»ƒ
bash examples/my_training.sh
```

## âš™ï¸ å…³é”®é…ç½®è¯´æ˜

### GPU é…ç½®

```bash
# GPU æ•°é‡
NUM_GPUS=4

# æŒ‡å®šä½¿ç”¨å“ªäº› GPUï¼ˆå¯é€‰ï¼‰
export CUDA_VISIBLE_DEVICES=0,1,2,3

# æ··åˆç²¾åº¦æ¨¡å¼
MIXED_PRECISION="bf16"  # æ¨èç”¨äº A100, RTX 3090+
# MIXED_PRECISION="fp16"  # ç”¨äºè€æ˜¾å¡å¦‚ V100
# MIXED_PRECISION="no"    # ç¦ç”¨æ··åˆç²¾åº¦
```

### Batch Size

```bash
# æ¯ä¸ª GPU çš„ batch size
BATCH_SIZE=16

# æœ‰æ•ˆ batch size = BATCH_SIZE Ã— NUM_GPUS
# ä¾‹å¦‚ï¼š16 Ã— 4 = 64
```

**é‡è¦**ï¼šå¦‚æœä»å• GPU è¿ç§»åˆ°å¤š GPUï¼š
- å• GPU: `batch_size=64` â†’ 4 GPU: `batch_size=16` (ä¿æŒæœ‰æ•ˆ batch size = 64)

### Learning Rate

è„šæœ¬ä½¿ç”¨ç­–ç•¥çš„é»˜è®¤å­¦ä¹ ç‡ã€‚å¦‚æœæ”¹å˜æœ‰æ•ˆ batch sizeï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ï¼š

```bash
# åœ¨è„šæœ¬ä¸­æ·»åŠ 
--optimizer.lr=2e-4  # æ ¹æ®æœ‰æ•ˆ batch size è°ƒæ•´
```

### Workers æ•°é‡

```bash
# æ¯ä¸ª GPU çš„ data loading workers
NUM_WORKERS=8

# æ¨èå€¼ï¼š4-8 workers per GPU
# æ€» workers = NUM_WORKERS Ã— NUM_GPUS
```

## ğŸ“Š ä½¿ç”¨åœºæ™¯ç¤ºä¾‹

### åœºæ™¯ 1: è®­ç»ƒ SmolVLAï¼ˆ4 GPUï¼‰

```bash
# ä½¿ç”¨ train_smolvla_multi_gpu.sh
# ä¿®æ”¹æ•°æ®é›†è·¯å¾„å’Œè¾“å‡ºç›®å½•åè¿è¡Œ
bash examples/train_smolvla_multi_gpu.sh
```

### åœºæ™¯ 2: è®­ç»ƒ ACT Policyï¼ˆ2 GPUï¼‰

```bash
# å¤åˆ¶æ¨¡æ¿
cp examples/train_multi_gpu_template.sh examples/train_act_2gpu.sh

# ä¿®æ”¹é…ç½®
# POLICY_PATH="act"
# NUM_GPUS=2
# BATCH_SIZE=32
# DATASET_REPO_ID="your-username/your-aloha-dataset"

# è¿è¡Œ
bash examples/train_act_2gpu.sh
```

### åœºæ™¯ 3: ä½¿ç”¨ç‰¹å®š GPU

```bash
# åªä½¿ç”¨ GPU 2 å’Œ 3
export CUDA_VISIBLE_DEVICES=2,3

# ä¿®æ”¹è„šæœ¬ä¸­çš„ NUM_GPUS=2
bash examples/train_multi_gpu_template.sh
```

### åœºæ™¯ 4: ä» Checkpoint æ¢å¤è®­ç»ƒ

```bash
# åœ¨è„šæœ¬ä¸­æ·»åŠ 
--resume=true \
--checkpoint_path=outputs/20250115_123456/checkpoints/last
```

## ğŸ” ç›‘æ§è®­ç»ƒ

### å®æ—¶æŸ¥çœ‹ GPU ä½¿ç”¨

```bash
# æ–¹æ³• 1: nvidia-smi
watch -n 1 nvidia-smi

# æ–¹æ³• 2: gpustatï¼ˆéœ€è¦å®‰è£…ï¼‰
pip install gpustat
gpustat -i 1
```

### WandB ç›‘æ§

è„šæœ¬å·²é…ç½® WandB æ—¥å¿—è®°å½•ã€‚è®­ç»ƒå¼€å§‹åï¼ŒæŸ¥çœ‹é“¾æ¥ï¼š
```
https://wandb.ai/your-entity/your-project
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. æ˜¾å­˜ä¸è¶³ (OOM)

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å‡å°æ¯ä¸ª GPU çš„ batch size
BATCH_SIZE=8  # ä» 16 é™åˆ° 8

# æˆ–å‡å°‘ workers
NUM_WORKERS=4  # ä» 8 é™åˆ° 4
```

### 2. NCCL é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨è„šæœ¬ä¸­æ·»åŠ 
```bash
export NCCL_P2P_DISABLE="1"
export NCCL_IB_DISABLE="1"
export NCCL_DEBUG=INFO
```

### 3. GPU åˆ©ç”¨ç‡ä½

**æ£€æŸ¥**ï¼š
```bash
# æŸ¥çœ‹ GPU åˆ©ç”¨ç‡
nvidia-smi

# å¦‚æœ < 80%ï¼Œå°è¯•ï¼š
# 1. å¢å¤§ BATCH_SIZE
# 2. å¢å¤§ NUM_WORKERS
# 3. æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦æ˜¯ç“¶é¢ˆ
```

### 4. è®­ç»ƒé€Ÿåº¦æ²¡æœ‰æå‡

**å¯èƒ½åŸå› **ï¼š
- Batch size å¤ªå°ï¼ˆå¢å¤§ BATCH_SIZEï¼‰
- Workers ä¸è¶³ï¼ˆå¢å¤§ NUM_WORKERSï¼‰
- ç½‘ç»œé€šä¿¡ç“¶é¢ˆï¼ˆæ£€æŸ¥ GPU äº’è”ï¼‰

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å¼€å¯æ··åˆç²¾åº¦

```bash
MIXED_PRECISION="bf16"  # å¯ä»¥è·å¾— 1.5-2Ã— åŠ é€Ÿ
```

### 2. ä¼˜åŒ– Batch Size

```bash
# æ‰¾åˆ°æœ€å¤§å¯ç”¨çš„ batch sizeï¼ˆä¸ OOMï¼‰
# ä»å°å¼€å§‹æµ‹è¯•ï¼š8, 16, 24, 32...
BATCH_SIZE=24  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
```

### 3. è°ƒæ•´ Workers

```bash
# CPU æ ¸å¿ƒå……è¶³æ—¶
NUM_WORKERS=8  # æ¯ GPU 8 ä¸ª workers

# CPU æ ¸å¿ƒæœ‰é™æ—¶
NUM_WORKERS=4  # æ¯ GPU 4 ä¸ª workers
```

### 4. ä½¿ç”¨æ›´å¿«çš„å­˜å‚¨

å¦‚æœæ•°æ®é›†åœ¨æ…¢é€Ÿå­˜å‚¨ä¸Šï¼Œè€ƒè™‘ï¼š
- å°†æ•°æ®é›†å¤åˆ¶åˆ°æœ¬åœ° SSD
- ä½¿ç”¨æ›´å¿«çš„ç½‘ç»œå­˜å‚¨
- å¢åŠ  prefetch_factor

## ğŸ¯ æœ€ä½³å®è·µ

1. **ä¿æŒæœ‰æ•ˆ batch size ä¸€è‡´**
   ```bash
   # å• GPU: batch_size=64
   # 4 GPU: batch_size=16 (æœ‰æ•ˆ = 64)
   ```

2. **ä»å°è§„æ¨¡å¼€å§‹æµ‹è¯•**
   ```bash
   # å…ˆç”¨å°‘é‡ steps æµ‹è¯•é…ç½®
   STEPS=100
   # ç¡®è®¤æ— è¯¯åå†è¿›è¡Œå®Œæ•´è®­ç»ƒ
   ```

3. **ç›‘æ§æ‰€æœ‰ GPU**
   ```bash
   # ç¡®ä¿æ‰€æœ‰ GPU åˆ©ç”¨ç‡ç›¸è¿‘
   watch -n 1 nvidia-smi
   ```

4. **ä½¿ç”¨ WandB è¿½è¸ªå®éªŒ**
   ```bash
   ENABLE_WANDB=true
   WANDB_RUN_ID="run_$(date +%Y%m%d_%H%M%S)"
   ```

5. **å®šæœŸä¿å­˜ checkpoint**
   ```bash
   --save_freq=1000  # æ¯ 1000 æ­¥ä¿å­˜ä¸€æ¬¡
   ```

## ğŸ“š æ›´å¤šèµ„æº

- [å®Œæ•´å¤šå¡è®­ç»ƒæŒ‡å—](../docs/MULTI_GPU_TRAINING.md)
- [å¿«é€Ÿå…¥é—¨æŒ‡å—](../MULTI_GPU_QUICKSTART.md)
- [Accelerate æ–‡æ¡£](https://huggingface.co/docs/accelerate)

## ğŸ’¡ æç¤º

- ä¿®æ”¹è„šæœ¬å‰å…ˆå¤‡ä»½ï¼š`cp script.sh script.sh.bak`
- ä½¿ç”¨ `set -e` ç¡®ä¿å‡ºé”™æ—¶è„šæœ¬åœæ­¢
- ä½¿ç”¨æœ‰æ„ä¹‰çš„ `RUN_ID` ä¾¿äºè¿½è¸ªå®éªŒ
- å®šæœŸæ£€æŸ¥è¾“å‡ºç›®å½•çš„ç£ç›˜ç©ºé—´
