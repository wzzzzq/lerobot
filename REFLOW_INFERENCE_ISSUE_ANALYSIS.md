# Reflowæ¨ç†é—®é¢˜å®Œæ•´åˆ†æ

## é—®é¢˜æè¿°
Reflowè®­ç»ƒçš„æ¨¡å‹åœ¨robotwin evalæ—¶åŠ¨ä½œå®Œå…¨ä¸åƒteacheræ¨¡å‹ï¼Œå®Œå…¨æ˜¯ä¹±åŠ¨ã€‚

## å·²ç¡®è®¤çš„ä¸€è‡´æ€§
1. âœ… **num_steps**: teacherå’Œstudentéƒ½æ˜¯10æ­¥
2. âœ… **ODEç§¯åˆ†æ–¹å‘**: éƒ½æ˜¯ä»t=1ç§¯åˆ†åˆ°t=0
3. âœ… **å›¾åƒå½’ä¸€åŒ–**: preprocessorä¸åšå›¾åƒå½’ä¸€åŒ–ï¼Œåªæœ‰prepare_imagesåšï¼ˆ[0,1] -> [-1,1]ï¼‰
4. âœ… **å™ªå£°é‡‡æ ·**: éƒ½ä½¿ç”¨torch.randn/torch.normalä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·

## å‘ç°çš„å…³é”®ä»£ç è·¯å¾„å·®å¼‚

### 1. Trainingæ—¶teacherç”ŸæˆX0çš„æµç¨‹

```python
# lerobot_train_reflow.py::prepare_reflow_batch
# 1. Batchç»è¿‡preprocessorï¼ˆnormalize state/actionï¼Œä¸normalize imageï¼‰
batch = pre_processor(batch)

# 2. åœ¨prepare_reflow_batchä¸­
if student.config.adapt_to_pi_aloha:
    batch[OBS_STATE] = student._pi_aloha_decode_state(batch[OBS_STATE])
    batch[ACTION] = student._pi_aloha_encode_actions_inv(batch[ACTION])

# 3. é¢„å¤„ç†observations
images, img_masks = student.prepare_images(batch)  # image: [0,1] -> [-1,1]
state = student.prepare_state(batch)  # padding
lang_tokens = batch[f"{OBS_LANGUAGE_TOKENS}"]
lang_masks = batch[f"{OBS_LANGUAGE_ATTENTION_MASK}"]

# 4. é‡‡æ ·å™ªå£°
X_1 = torch.randn(action_shape, device=device, dtype=dtype)  # dtypeæ¥è‡ªbatch["action"]
X_1_padded = pad_vector(X_1, teacher.config.max_action_dim)

# 5. Teacherç”ŸæˆX0
with torch.no_grad():
    teacher.eval()
    X_0 = teacher.model.sample_actions(
        images, img_masks, lang_tokens, lang_masks, state, noise=X_1_padded
    )
    X_0 = X_0[:, :, :original_action_dim]  # unpad
```

### 2. Trainingæ—¶studentçš„forward

```python
# lerobot_train_reflow.py::mainè®­ç»ƒå¾ªç¯
policy.train()  # â— studentæ˜¯trainæ¨¡å¼
losses = policy.model.forward(
    images, img_masks, lang_tokens, lang_masks, state, X_0_padded, noise=X_1_padded, time=None
)
```

**Forwardæ–¹æ³•å†…éƒ¨** (modeling_smolvla.py:671-707):
```python
def forward(self, images, img_masks, lang_tokens, lang_masks, state, actions, noise=None, time=None):
    if noise is None:
        noise = self.sample_noise(actions.shape, actions.device)

    if time is None:
        time = self.sample_time(actions.shape[0], actions.device)  # éšæœºé‡‡æ ·æ—¶é—´

    time_expanded = time[:, None, None]
    x_t = time_expanded * noise + (1 - time_expanded) * actions  # æ’å€¼
    u_t = noise - actions  # ç›®æ ‡velocity

    # åµŒå…¥prefixå’Œsuffix
    prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(...)
    suffix_embs, suffix_pad_masks, suffix_att_masks = self.embed_suffix(x_t, time)

    # Forward pass
    (_, suffix_out), _ = self.vlm_with_expert.forward(...)

    # è®¡ç®—loss
    v_t = self.action_out_proj(suffix_out)
    losses = F.mse_loss(u_t, v_t, reduction="none")
    return losses
```

### 3. Evalæ—¶studentçš„æ¨ç†

```python
# eval_policy_smolvla.py::get_action
# 1. Observationç»è¿‡preprocessor
self.observation_window = self.preprocessor(observation)

# 2. Policyæ¨ç†
@torch.no_grad()
def select_action(self, batch):
    self.eval()  # â— studentæ˜¯evalæ¨¡å¼
    batch = self._prepare_batch(batch)
    ...
    actions = self._get_action_chunk(batch, noise=None)  # noise=Noneï¼Œä¼šéšæœºç”Ÿæˆ
    ...
```

**sample_actionsæ–¹æ³•å†…éƒ¨** (modeling_smolvla.py:709-748):
```python
def sample_actions(self, images, img_masks, lang_tokens, lang_masks, state, noise=None):
    bsize = state.shape[0]
    device = state.device

    if noise is None:
        actions_shape = (bsize, self.config.chunk_size, self.config.max_action_dim)
        noise = self.sample_noise(actions_shape, device)  # éšæœºç”Ÿæˆå™ªå£°

    # åµŒå…¥prefixï¼ˆKV cacheï¼‰
    prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(...)
    _, past_key_values = self.vlm_with_expert.forward(..., fill_kv_cache=True)

    # ODEç§¯åˆ†
    dt = -1.0 / self.config.num_steps
    x_t = noise  # ä»t=1å¼€å§‹
    time = torch.tensor(1.0, ...)

    while time >= -dt / 2:
        expanded_time = time.expand(bsize)
        v_t = self.denoise_step(prefix_pad_masks, past_key_values, x_t, expanded_time)
        x_t += dt * v_t  # Eulerç§¯åˆ†
        time += dt

    return x_t  # è¿”å›t=0æ—¶çš„action
```

## ğŸ”´ **å‘ç°çš„æ ¸å¿ƒé—®é¢˜**

### é—®é¢˜1ï¼šæ¨¡å‹æ¨¡å¼ä¸ä¸€è‡´

| é˜¶æ®µ | Teacheræ¨¡å¼ | Studentæ¨¡å¼ |
|------|------------|------------|
| Trainingç”ŸæˆX0 | eval() | train() |
| Evalæ¨ç† | N/A | eval() |

**Trainingæ—¶**ï¼š
- Teacher: `teacher.eval()` â†’ **æ— dropout, batchnormä½¿ç”¨running stats**
- Student: `policy.train()` â†’ **å¯èƒ½æœ‰dropout, batchnormä½¿ç”¨batch stats**

**Evalæ—¶**ï¼š
- Student: `policy.eval()` â†’ **æ— dropout, batchnormä½¿ç”¨running stats**

**å½±å“**ï¼šå¦‚æœstudentçš„expertæˆ–å…¶ä»–å±‚æœ‰dropoutæˆ–batchnormï¼Œè®­ç»ƒå’Œæ¨ç†æ—¶çš„è¡Œä¸ºä¼šä¸åŒï¼

### é—®é¢˜2ï¼šå™ªå£°dtypeå¯èƒ½ä¸ä¸€è‡´

```python
# Trainingæ—¶
dtype = batch["action"].dtype  # å¯èƒ½æ˜¯float16æˆ–float32
X_1 = torch.randn(..., dtype=dtype)

# Evalæ—¶ (sample_noise)
noise = torch.normal(..., dtype=torch.float32)  # å›ºå®šfloat32
```

### é—®é¢˜3ï¼šæ—¶é—´é‡‡æ ·çš„å·®å¼‚

**Trainingæ—¶**ï¼š
```python
# forwardä¸­
if time is None:
    time = self.sample_time(bsize, device)  # Beta(1.5, 1.0)åˆ†å¸ƒï¼ŒèŒƒå›´[0.001, 1.0]

# sample_timeå®ç°
def sample_time(self, bsize, device):
    beta_dist = torch.distributions.Beta(concentration1=1.5, concentration0=1.0)
    time_beta = beta_dist.sample((bsize,)).to(device=device, dtype=torch.float32)
    time = time_beta * 0.999 + 0.001
    return time
```

**Evalæ—¶** (sample_actions):
```python
# ODEç§¯åˆ†ä½¿ç”¨å›ºå®šçš„æ—¶é—´åºåˆ—
time = torch.tensor(1.0, ...)  # ä»1.0å¼€å§‹
while time >= -dt / 2:
    ...
    time += dt  # é€’å‡
```

## âš ï¸ **æœ€å¯ç–‘çš„é—®é¢˜**

### **é—®é¢˜4ï¼šStudentè®­ç»ƒæ—¶æ˜¯trainæ¨¡å¼ï¼Œä½†åº”è¯¥æ˜¯evalæ¨¡å¼ï¼**

åœ¨reflowè®­ç»ƒä¸»å¾ªç¯ä¸­ï¼š
```python
policy.train()  # âŒ è¿™é‡Œè®¾ç½®ä¸ºtrainæ¨¡å¼
losses = policy.model.forward(...)
```

ä½†æ˜¯teacherç”ŸæˆX0æ—¶æ˜¯evalæ¨¡å¼ï¼š
```python
teacher.eval()
X_0 = teacher.model.sample_actions(...)
```

**åæœ**ï¼š
1. å¦‚æœstudentçš„expertæœ‰**dropout**ï¼Œè®­ç»ƒæ—¶ä¼šéšæœºä¸¢å¼ƒç¥ç»å…ƒï¼Œä½†æ¨ç†æ—¶ä¸ä¼š
2. å¦‚æœstudentçš„expertæœ‰**batchnorm**ï¼Œè®­ç»ƒæ—¶ä½¿ç”¨batch statisticsï¼Œæ¨ç†æ—¶ä½¿ç”¨running statistics
3. Studentå­¦åˆ°çš„æ˜¯"å¸¦dropoutçš„é¢„æµ‹"ï¼Œä½†æ¨ç†æ—¶æ²¡æœ‰dropoutï¼Œå¯¼è‡´è¾“å‡ºåˆ†å¸ƒå®Œå…¨ä¸åŒï¼

## ğŸ¯ **éªŒè¯æ­¥éª¤**

### 1. æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰dropout/batchnorm

è¿è¡Œä»¥ä¸‹ä»£ç æ£€æŸ¥studentæ¨¡å‹ï¼š
```python
for name, module in student.named_modules():
    if isinstance(module, (torch.nn.Dropout, torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):
        print(f"Found: {name} -> {module}")
```

### 2. æ£€æŸ¥è®­ç»ƒæ—¶çš„dtype

åœ¨prepare_reflow_batchä¸­æ·»åŠ ï¼š
```python
print(f"[DEBUG] batch['action'].dtype = {batch['action'].dtype}")
print(f"[DEBUG] X_1.dtype = {X_1.dtype}")
```

### 3. éªŒè¯teacherå’Œstudentçš„num_steps

```python
print(f"Teacher num_steps: {teacher.config.num_steps}")
print(f"Student num_steps: {student.config.num_steps}")
```

## ğŸ’¡ **å»ºè®®çš„ä¿®å¤æ–¹æ¡ˆ**

### æ–¹æ¡ˆ1ï¼šç»Ÿä¸€æ¨¡å‹æ¨¡å¼ï¼ˆæ¨èï¼‰

åœ¨reflowè®­ç»ƒå¾ªç¯ä¸­ï¼š
```python
# ä¿®æ”¹å‰
policy.train()
losses = policy.model.forward(...)

# ä¿®æ”¹å
policy.eval()  # âœ… ä½¿ç”¨evalæ¨¡å¼ï¼Œå’Œteacherä¸€è‡´
with torch.no_grad():
    # Forward passä¸éœ€è¦æ¢¯åº¦
    losses = policy.model.forward(...)

# Loss backwardä»ç„¶éœ€è¦æ¢¯åº¦
loss = losses.mean()
loss.requires_grad = True  # ç¡®ä¿å¯ä»¥backward
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

**ç­‰ç­‰ï¼Œè¿™æ ·ä¸å¯¹ï¼Forwardéœ€è¦è®¡ç®—æ¢¯åº¦æ‰èƒ½backwardã€‚**

æ­£ç¡®çš„æ–¹æ¡ˆï¼š
```python
# ä¿®æ”¹ï¼šåœ¨forwardä¹‹å‰è®¾ç½®evalæ¨¡å¼ï¼Œä½†ä¸ä½¿ç”¨no_grad
policy.eval()  # âœ… è®¾ç½®ä¸ºevalæ¨¡å¼ï¼Œç¦ç”¨dropout
losses = policy.model.forward(...)  # ä»ç„¶ä¿ç•™æ¢¯åº¦è®¡ç®—

# æ­£å¸¸backward
loss = losses.mean()
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

### æ–¹æ¡ˆ2ï¼šç¡®ä¿expertæ²¡æœ‰dropout

æ£€æŸ¥expertçš„å®šä¹‰ï¼Œç¡®ä¿è®­ç»ƒæ—¶ä¸ä½¿ç”¨dropoutï¼Œæˆ–è€…åœ¨forwardæ—¶æ‰‹åŠ¨ç¦ç”¨ã€‚

### æ–¹æ¡ˆ3ï¼šç»Ÿä¸€noiseçš„dtype

```python
# åœ¨prepare_reflow_batchä¸­å¼ºåˆ¶ä½¿ç”¨float32
X_1 = torch.randn(action_shape, device=device, dtype=torch.float32)
```

## ğŸ“‹ **æ€»ç»“**

æœ€å¯èƒ½çš„é—®é¢˜æ˜¯**studentè®­ç»ƒæ—¶ä½¿ç”¨trainæ¨¡å¼ï¼Œå¯¼è‡´dropout/batchnormè¡Œä¸ºå’Œevalæ—¶ä¸ä¸€è‡´**ã€‚

å»ºè®®ï¼š
1. âœ… ç«‹å³æ£€æŸ¥expertæ˜¯å¦æœ‰dropoutå±‚
2. âœ… ä¿®æ”¹è®­ç»ƒå¾ªç¯ï¼Œåœ¨forwardä¹‹å‰è°ƒç”¨`policy.eval()`
3. âœ… éªŒè¯ä¿®æ”¹åæ¨¡å‹åœ¨evalæ—¶çš„è¡Œä¸ºæ˜¯å¦æ­£å¸¸
